{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lenet-mnist-swift-models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "swift",
      "display_name": "Swift"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbolella/s4tf-lenet-mnist/blob/master/lenet_mnist_swift_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TQjRmPLaGT2",
        "colab_type": "text"
      },
      "source": [
        "# LeNet-5 & MNIST using Swift for Tensorflow\n",
        "by Danny Bolella\n",
        "\n",
        "To learn more about how this Colab works, check out the associated Medium article at: \n",
        "\n",
        "This Colab is a reworking of the official S4TF Example found at: https://github.com/tensorflow/swift-models/tree/master/Examples/LeNet-MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MckWj_xja0Ru",
        "colab_type": "text"
      },
      "source": [
        "## Installing and Importing Libraries\n",
        "First, we pull in 2 libraries as swift packages from the official S4TF models repo.  We use `%install` to accomplish this.  Once complete, we then import the libraries we'll be using (Tensorflow is already available on Colab, no need to install)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDnsjoqvB1g_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%install '.package(url: \"https://github.com/tensorflow/swift-models.git\", .branch(\"master\"))' ImageClassificationModels Datasets\n",
        "\n",
        "import TensorFlow\n",
        "import Datasets\n",
        "import ImageClassificationModels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR-XcJygbCM4",
        "colab_type": "text"
      },
      "source": [
        "## Model, Dataset, Optimizer... Oh My!\n",
        "Next, we instantiate the dataset, model, and optimizer we will be using.  We also setup our epochCount (the number of times we'll train our model) and batchSize (how much data we'll train with at a time).\n",
        "\n",
        "One last thing to do is setup our test data into batches using our designated batchSize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN5sL1KJFk-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "let dataset = MNIST()\n",
        "\n",
        "var model = LeNet()\n",
        "\n",
        "let optimizer = SGD(for: model, learningRate: 0.1)\n",
        "\n",
        "let epochCount = 12\n",
        "\n",
        "let batchSize = 128\n",
        "\n",
        "let testBatches = dataset.testDataset.batched(batchSize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo6dX8-cdM4g",
        "colab_type": "text"
      },
      "source": [
        "## Benchmarking Prep\n",
        "Lastly, we create a `struct` that we will use to hold our training and testing benchmarks per epoch.  Note that we also have a function in our struct to update our `GuessCount` stats.  This eliminates duplicate code in our training and testing loops."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDZZ_NMxdNSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "struct Statistics {\n",
        "    var correctGuessCount: Int = 0\n",
        "    var totalGuessCount: Int = 0\n",
        "    var totalLoss: Float = 0\n",
        "    \n",
        "    mutating func updateGuessCounts(logits: Tensor<Float>, labels: Tensor<Int32>, batchSize: Int) {\n",
        "      let correctPredictions = logits.argmax(squeezingAxis: 1) .== labels\n",
        "      self.correctGuessCount += Int(\n",
        "            Tensor<Int32>(correctPredictions).sum().scalarized())\n",
        "      self.totalGuessCount += batchSize\n",
        "    }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErIX2Bc7bUly",
        "colab_type": "text"
      },
      "source": [
        "## Training Day\n",
        "Lastly, we run our training!  We run the training loop based on our `epochCount`.  Each time we do, we loop through batches of our data, run it through our model, update our benchmarks, and optimize along the gradients.  \n",
        "\n",
        "At the end of each epoch, we print out our benchmark data.  We should see our loss decrease and our accuracy increase with each pass of training our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nfcjc7vYlcpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Beginning training...\")\n",
        "\n",
        "// The training loop.\n",
        "for epoch in 1...epochCount {\n",
        "    var trainStats = Statistics()\n",
        "    var testStats = Statistics()\n",
        "    let trainingShuffled = dataset.trainingDataset.shuffled(\n",
        "        sampleCount: dataset.trainingExampleCount, randomSeed: Int64(epoch))\n",
        "    Context.local.learningPhase = .training\n",
        "    for batch in trainingShuffled.batched(batchSize) {\n",
        "        let (labels, images) = (batch.label, batch.data)\n",
        "        // Compute the gradient with respect to the model.\n",
        "        let (loss, gradients) = valueWithGradient(at: model) { model -> Tensor<Float> in\n",
        "            let logits = model(images)\n",
        "            trainStats.updateGuessCounts(logits: logits, labels: labels, batchSize: batchSize)\n",
        "            return softmaxCrossEntropy(logits: logits, labels: labels)\n",
        "        }\n",
        "        trainStats.totalLoss += loss.scalarized()\n",
        "        optimizer.update(&model, along: gradients)\n",
        "    }\n",
        "\n",
        "    Context.local.learningPhase = .inference\n",
        "    for batch in testBatches {\n",
        "        let (labels, images) = (batch.label, batch.data)\n",
        "        // Compute loss on test set\n",
        "        let logits = model(images)\n",
        "        testStats.updateGuessCounts(logits: logits, labels: labels, batchSize: batchSize)\n",
        "        let loss = softmaxCrossEntropy(logits: logits, labels: labels)\n",
        "        testStats.totalLoss += loss.scalarized()\n",
        "    }\n",
        "\n",
        "    let trainAccuracy = Float(trainStats.correctGuessCount) / Float(trainStats.totalGuessCount)\n",
        "    let testAccuracy = Float(testStats.correctGuessCount) / Float(testStats.totalGuessCount)\n",
        "    print(\"\"\"\n",
        "          [Epoch \\(epoch)] \\\n",
        "          Training Loss: \\(trainStats.totalLoss), \\\n",
        "          Training Accuracy: \\(trainStats.correctGuessCount)/\\(trainStats.totalGuessCount) \\\n",
        "          (\\(trainAccuracy)), \\\n",
        "          Test Loss: \\(testStats.totalLoss), \\\n",
        "          Test Accuracy: \\(testStats.correctGuessCount)/\\(testStats.totalGuessCount) \\\n",
        "          (\\(testAccuracy))\n",
        "          \"\"\")\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbrylhslosv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}